{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>December 2020 - This notebook was created by [Oriol Pujol Vila](http://www.maia.ub.es/~oriol). Source and [license](./LICENSE.txt) info are in the folder.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning (in pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Autoencoders\n",
    "+ Pretraining\n",
    "+ Manifold learning\n",
    "+ Sparse coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5154, 0.7644, 0.4711],\n",
      "        [0.3706, 0.8213, 0.5367],\n",
      "        [0.0985, 0.8863, 0.9981],\n",
      "        [0.9071, 0.8590, 0.3166],\n",
      "        [0.6586, 0.3844, 0.0997]])\n"
     ]
    }
   ],
   "source": [
    "#verify torch\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "data = load_digits()\n",
    "\n",
    "idx = np.random.permutation(data.data.shape[0])\n",
    "idx_train = idx[:-100]\n",
    "idx_test = idx[-100:]\n",
    "\n",
    "train = torch.from_numpy(data.data[idx_train,:]).float()\n",
    "test = torch.from_numpy(data.data[idx_test,:]).float()\n",
    "\n",
    "train_y = data.target[idx_train]\n",
    "test_y = data.target[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class myfirstautoencoder(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__()\n",
    "        self.d_input = kwargs[\"dim\"]\n",
    "        self.dr1 = nn.Dropout(p=0.2)\n",
    "        self.e1 = nn.Linear(self.d_input,64)\n",
    "        self.e2 = nn.Linear(64,32)\n",
    "        self.e3 = nn.Linear(32,10)\n",
    "        self.d1 = nn.Linear(10,32)\n",
    "        self.d2 = nn.Linear(32,64)\n",
    "        self.d3 = nn.Linear(64,self.d_input)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.dr1(x)\n",
    "        x = F.relu(self.e1(x))\n",
    "        x = F.relu(self.e2(x))\n",
    "        x = self.bottleneck = F.relu(self.e3(x))\n",
    "        x = F.relu(self.d1(x))\n",
    "        x = F.relu(self.d2(x))\n",
    "        prediction = self.d3(x)\n",
    "        return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = myfirstautoencoder(dim = 64)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(32,64)\n",
    "\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 15000\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "for i in range(iters):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    idx = np.random.randint(0,train.shape[0],size = 32)\n",
    "    inputs = train[idx,:]\n",
    "    labels = inputs\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 200 == 199:    # print every 200 iterations\n",
    "        print('[%5d] loss: %.3f' %\n",
    "              (i + 1, running_loss))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model:\n",
    "net.eval()\n",
    "k= 101\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstruction = net(train[k:k+1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t=train[k:k+1,:].numpy()\n",
    "#r=reconstruction.detach().numpy()\n",
    "r=reconstruction.numpy()\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(t.reshape((8,8)),cmap=\"gray\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(r.reshape((8,8)),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.forward(train)\n",
    "code = net.bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(5,random_state=0)\n",
    "clf.fit(code.detach().numpy(),train_y)\n",
    "\n",
    "clf2 = RandomForestClassifier(5,random_state=0)\n",
    "clf2.fit(train.detach().numpy(),train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.forward(test)\n",
    "code = net.bottleneck\n",
    "\n",
    "print(\"Inner representation: \" + str(clf.score(code.detach().numpy(),test_y)))\n",
    "print(\"Original data: \" + str(clf2.score(test.detach().numpy(),test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# Load data\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "data = load_digits()\n",
    "\n",
    "idx = np.random.permutation(data.data.shape[0])\n",
    "idx_train = idx[:-100]\n",
    "idx_test = idx[-100:]\n",
    "\n",
    "train = torch.from_numpy(data.data[idx_train,:]).float()\n",
    "test = torch.from_numpy(data.data[idx_test,:]).float()\n",
    "\n",
    "train_y = data.target[idx_train]\n",
    "test_y = data.target[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class myfirstautoencoder(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__()\n",
    "        self.d_input = kwargs[\"dim\"]\n",
    "        self.dr1 = nn.Dropout(p=0.2)\n",
    "        self.e1 = nn.Linear(self.d_input,64)\n",
    "        self.dr2 = nn.Dropout(p=0.2)\n",
    "        self.e2 = nn.Linear(64,128)\n",
    "        self.e3 = nn.Linear(128,256)\n",
    "        self.d1 = nn.Linear(256,128)\n",
    "        self.d2 = nn.Linear(128,64)\n",
    "        self.d3 = nn.Linear(64,self.d_input)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.dr1(x)\n",
    "        x = F.relu(self.e1(x))\n",
    "        x = self.dr2(x)\n",
    "        x = F.relu(self.e2(x))\n",
    "        bottleneck = F.relu(self.e3(x))\n",
    "        x = F.relu(self.d1(bottleneck))\n",
    "        x = F.relu(self.d2(x))\n",
    "        prediction = self.d3(x)\n",
    "        return prediction,bottleneck\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = myfirstautoencoder(dim = 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "\n",
    "iters = 30000\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "for i in range(iters):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    idx = np.random.randint(0,train.shape[0],size = 32)\n",
    "    inputs = train[idx,:]\n",
    "    labels = inputs\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs,bn = net(inputs)\n",
    "    loss = criterion(outputs, labels) + 0.001*torch.norm(bn,1)\n",
    "    #loss += 0.00000001*torch.norm(bn,1)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 200 == 199:    # print every 200 iterations\n",
    "        print('[%5d] loss: %.3f' %\n",
    "              (i + 1, running_loss))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model:\n",
    "net.eval()\n",
    "k= 101\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstruction,spar = net(train[k:k+1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "t=train[k:k+1,:].numpy()\n",
    "r=reconstruction.detach().numpy()\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(t.reshape((8,8)),cmap=\"gray\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(r.reshape((8,8)),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(spar[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "data = load_digits()\n",
    "\n",
    "idx = np.random.permutation(data.data.shape[0])\n",
    "idx_train = idx[:-100]\n",
    "idx_test = idx[-100:]\n",
    "\n",
    "train = torch.from_numpy(data.data[idx_train,:]).float()\n",
    "test = torch.from_numpy(data.data[idx_test,:]).float()\n",
    "\n",
    "train_y = data.target[idx_train]\n",
    "test_y = data.target[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class myfirstautoencoder(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__()\n",
    "        self.d_input = kwargs[\"dim\"]\n",
    "        self.dr1 = nn.Dropout(p=0.2)\n",
    "        self.e1 = nn.Linear(self.d_input,64)\n",
    "        self.e2 = nn.Linear(64,32)\n",
    "        self.e3 = nn.Linear(32,16)\n",
    "        self.e4 = nn.Linear(16,2)\n",
    "        self.d1 = nn.Linear(2,16)\n",
    "        self.d2 = nn.Linear(16,32)\n",
    "        self.d3 = nn.Linear(32,64)\n",
    "        self.d4 = nn.Linear(64,self.d_input)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.dr1(x)\n",
    "        x = F.relu(self.e1(x))\n",
    "        x = F.relu(self.e2(x))\n",
    "        x = F.relu(self.e3(x))\n",
    "        x = bottleneck = F.relu(self.e4(x))\n",
    "        x = F.relu(self.d1(x))\n",
    "        x = F.relu(self.d2(x))\n",
    "        x = F.relu(self.d3(x))\n",
    "        prediction = self.d4(x)\n",
    "        return prediction,bottleneck\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = myfirstautoencoder(dim = 64)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net.train()\n",
    "\n",
    "iters = 30000\n",
    "\n",
    "\n",
    "running_loss = 0.0\n",
    "for i in range(iters):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    idx = np.random.randint(0,train.shape[0],size = 32)\n",
    "    inputs = train[idx,:]\n",
    "    labels = inputs\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs,bn = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    #loss += 0.00000001*torch.norm(bn,1)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 200 == 199:    # print every 200 iterations\n",
    "        print('[%5d] loss: %.3f' %\n",
    "              (i + 1, running_loss))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model:\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstruction,representation = net(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = representation.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance \n",
    "N=10\n",
    "vx = np.linspace(np.min(representation[:,0]),np.max(representation[:,0]),N)\n",
    "vy = np.linspace(np.min(representation[:,1]),np.max(representation[:,1]),N)\n",
    "\n",
    "def is_visited(x,l):\n",
    "    for item in l:\n",
    "        if np.abs(x-item)<1e-10:\n",
    "            return True\n",
    "    return False\n",
    "visited=[]\n",
    "idx_mat=np.zeros((N,N))       \n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        d = distance.cdist(np.array([vx[i],vy[j]])[np.newaxis,:], representation)\n",
    "        idx_sort = np.argsort(d)[0]\n",
    "        idx_not_visited=[tmp for tmp in idx_sort if not(is_visited(tmp,visited))]\n",
    "        if len(idx_not_visited)>0:\n",
    "            idx_mat[i,j] = idx_not_visited[0]\n",
    "            visited.append(idx_not_visited[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(N, N)\n",
    "\n",
    "xs=train.numpy()\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        axarr[i,j].imshow(xs[int(idx_mat[i,j]),:].reshape((8,8)),cmap='gray', interpolation='nearest')\n",
    "f.set_size_inches(10,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
